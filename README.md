ğŸ“˜ HR Policy RAG Chatbot
(Flask + LangChain + FAISS + Ollama Gemma:2b â€” Fully Local AI Assistant)

This project is a fully local Retrieval-Augmented Generation (RAG) HR Policy Chatbot that answers questions strictly using the official VIT HR Manual. The entire system works offline, powered by:

ğŸ§  Ollama Gemma:2b (local LLM)

ğŸ” FAISS for vector similarity search

ğŸ“ LangChain for retrieval + guardrails

ğŸŒ Flask frontend with full chat UI

âš¡ 100% privacy safe â€” no API calls
âš¡ Works completely offline
âš¡ Uses a secure guardrail prompt to prevent hallucination

ğŸš€ Features
ğŸ§  Fully Local RAG Chatbot

Runs entirely on your machine â€” no cloud, no API keys.

ğŸ” Strong Guardrail Prompt

The bot answers only from context, never guesses.

ğŸ” FAISS Vector Database

Searches the HR manual at high speed.

ğŸ§ª Debug Retriever

Optionally prints retrieved chunks in console.

ğŸ’¬ Modern Chat Interface

Includes:

Chat history

Clear all chats

Typing animation

LocalStorage persistence

Responsive UI (dark mode)

ğŸ“ Project Structure
vit_hr_manual_bot_testing/
â”‚â”€â”€ app.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
â”‚â”€â”€ .gitignore

â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ logic/
â”‚   â”‚   â”œâ”€â”€ conversational_logic.py
â”‚   â”‚   â””â”€â”€ multi_query_logic.py
â”‚   â”œâ”€â”€ clean_and_prepare.py
â”‚   â”œâ”€â”€ build_index.py
â”‚   â””â”€â”€ start_chat.py

â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ HR_POLICY_merged.pdf
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ cleaned_policy.txt
â”‚   â””â”€â”€ vector_store/

â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html

â””â”€â”€ static/
    â”œâ”€â”€ script.js
    â””â”€â”€ style.css

ğŸ”§ Installation
1ï¸âƒ£ Install Ollama

Download â†’ https://ollama.com/download

Then pull the model:

ollama pull gemma:2b


Make sure Ollama is running:

ollama serve

2ï¸âƒ£ Create a Virtual Environment
python -m venv .venv


Activate it:

Windows
.venv\Scripts\activate

macOS/Linux
source .venv/bin/activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

ğŸ§¹ Step 1 â€” Clean the HR PDF

This generates a clean text file from the PDF.

python scripts/clean_and_prepare.py


Output saved at:

data/processed/cleaned_policy.txt

ğŸ§  Step 2 â€” Build the FAISS Vector Store
python scripts/build_index.py


Creates:

data/vector_store/

â–¶ Step 3 â€” Run the Chatbot (Flask)
python app.py


Visit:

http://127.0.0.1:5000

ğŸ§© How the RAG System Works
PDF â†’ Clean Text â†’ Chunking â†’ Embeddings â†’ FAISS Index â†’ Retriever â†’ LLM

Flow Diagram
HR Policy PDF
      â”‚
(clean_and_prepare.py)
      â–¼
Cleaned Text
      â”‚
(build_index.py)
      â–¼
FAISS Vector Store
      â”‚
User Query
      â–¼
Retriever (MMR)
      â–¼
Guardrail Prompt
      â–¼
Ollama Gemma 2b
      â–¼
Final Answer

ğŸ Debug Mode

Enable retrieved-chunk printing by setting:

DEBUG_RETRIEVAL = True

ğŸ§ª Test the Chain (without Flask)
from scripts.logic.conversational_logic import load_local_chain_with_guardrails

chain = load_local_chain_with_guardrails(
    llm_model_name="gemma:2b",
    embedding_model_name="sentence-transformers/all-MiniLM-L6-v2",
    vector_db_path="data/vector_store",
    debug_retrieval=True
)

print(chain.invoke({"query": "Explain paid leave"}))

âš  Troubleshooting
âŒ "Only one usage of each socket address"

Ollama already running â†’ do NOT run ollama serve twice.

âŒ "Missing some input keys: {'query'}"

Always call:

qa_chain.invoke({"query": "your question"})

âŒ FAISS not found

Run:

python scripts/build_index.py

âŒ Ollama connection error

Start Ollama:

ollama serve

ğŸ“¦ Requirements (from requirements.txt)
langchain
langchain-community
langchain-core
langchain-text-splitters
langchain-ollama
langchain-huggingface
sentence-transformers
faiss-cpu
pypdf
python-dotenv
Flask

ğŸ§ª Example Questions

â€œExplain paid leave policy.â€

â€œWhen does earned leave get encashed?â€

â€œWhat are rules for late attendance?â€

â€œWho approves manpower requisition?â€

ğŸ¤ Contributing

PRs and suggestions are welcome!

ğŸ“„ License

MIT License.